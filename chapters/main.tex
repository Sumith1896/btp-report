\chapter[ER Keyword Querying]{Entity Relationship Keyword Queries}

In this chapter, we present a detail discussion of our approach towards supporting keyword queries. Earlier part of the chapter introduces the query model illustrated with examples. A brief overview of the query processing mechanism follows. The latter half of the chapter focuses on query interpretation---a theme central to our approach. Finally, the discussion is closed with the description of how the result set of the query is produced.

\section{Query Model}

The notion of Entity Relationship Query (ERQ) or Shallow Semantic Query (SSQ) is defined by \cite{li2010structured, li2012entity} as a powerful and flexible query form which allows users to specify expected entity types or categories along with selection predicates for entities and relation predicates for relationship between entities.

For example, a query described as ``Find PERSON \emph{near} \underline{`Stanford graduate'}, and COMPANY \emph{near} \underline{`Silicon Valley'}, where PERSON \underline{founded} COMPANY" can be described in ERQ form as in Figure \ref{fig:erq}.

\begin{figure}[h!]
\centering
\begin{BVerbatim}
SELECT x, y
FROM PERSON x, COMPANY y
WHERE x:['Stanford', 'graduate']
WHERE y:["Silicon Valley"]
AND x,y:['found']
\end{BVerbatim}
\caption{An example ER Query}
\label{fig:erq}
\end{figure}

The query of Figure \ref{fig:erq} seeks the entity pairs \texttt{(x, y)} such that entity \texttt{x} is of type \texttt{PERSON} (and further a Stanford graduate), entity \texttt{y} is of type \texttt{COMPANY} (in particular, a Silicon Valley company) and \texttt{x} has founded \texttt{y}. \texttt{x} and \texttt{y} are entity variables that will bind to entities in the query result. \texttt{PERSON} and \texttt{COMPANY} describe the category/types of these entities. \texttt{"Stanford graduate"} and \texttt{"Silicon Valley"} play the role of selector keywords for entities \texttt{x} and \texttt{y} respectively. Finally, \texttt{found} is the relation keyword describing the relationship between entities \texttt{x} and \texttt{y}.

To make our discussion convenient, we formalize the notion of ERQ as follows. Our system expects the user to specify:

\begin{itemize}
\item The number of entities ($n$) desired in a result. The entity variables will be referred to as $e_1, e_2, ..., e_n$.
\item A list of category keywords $C_i$ for each entity variable $x_i$.
\item A list of \emph{selector keywords} $K_i$ for each entity variable $x_i$.
\item A list of relation keywords $R_{ij}$ describing the relationship between entities $x_i$ and $x_j$ for all $i \neq j$.
\end{itemize}

The result of the query shall be one or more $n$-tuples, where $n$ is as specified in the query. In particular, the elements of the tuple bind to entity variables $(e_1, e_2, ..., e_n)$.

The category keywords are used to specify what category or type the corresponding entity falls in. Selector keywords desire that the corresponding entity be \emph{associated} with these words. Relation keywords specify the relation predicates, in the same manner as ERQ. It is worth noting that each of $C_i$, $K_i$, and $R_{ij}$ may be left empty (unspecified) by the user. The user is also not expected to specify these query keywords in conformity with the schema of the queried RDF graph.

We now illustrate another example to ease understanding of our ERQ formalism. Suppose we want to find the parent-child pairs of US presidents who have attended the same university. Such a query is demanding three entities $e_1$, $e_2$, and $e_3$---$e_1, e_2$ are the presidents, and $e_3$ their common alma mater. A user may formulate the query in the following manner:

\begin{itemize}
\item Number of entities is 3.
\item $C_1=$\{`person'\}, $C_2=$\{`person'\}, and $C_3=$\{`university'\}.
\item $K_1=$\{`US', `President'\}, $K_2=$\{`US', 'President'\}, and $K_3=$\{\}.
\item $R_{12}=$\{`parent'\}, $R_{13}=$\{`education'\}, and $R_{23}=$\{`education'\}, 
\end{itemize}

As we observed in Chapter 3, the problem of segmenting a user keyword query into the keywords for type/categories of entities sought and those for describing relationship between these entities inherently limits the performance (in both precision and recall) of the querying system. The ER query model might seem like a convenient ignorance of such issues, and in many ways it is. Our stance is that ER queries do not expect the amount of user sophistication that SPARQL querying systems expect. It is a sweet compromise between SPARQL querying and free-keyword querying that allows us to focus on challenges beyond query segmentation. Further, we believe that our ideas in this work can be supplemented with a layer of query segmentation---enabling free-keyword querying. 

\subsection{Query Graph}
Any given ER query can be thought of as edge-labelled directed graph $G = (V, E)$, with the vertex set $V = \{e_1, e_2, ..., e_n\}$ and the 
labelled edge edge set $E = \{(e_i \xrightarrow{R_{ij}} e_j) \; | \; \forall \; R_{ij} \in \text{query} \}$. Answering the query would then 
mean finding subgraphs in the data that \emph{match} the query graph. In this direction, the ER keyword query model can be viewed as a 
relaxed variant of SPARQL query\footnote{The ER keyword query model is still very basic in comparison to SPARQL---which supports operations like sorting and aggregation that we do not consider here. Extensions of the ER keyword query model to support aggregation are left as future work.}, in which keywords can be used in place of actual schema elements of the data. While we do not explicitly use the graph interpretation of the query, except when a \emph{join} is required to produce the final results, it is still an important aid in visualizing the overall query processing mechanism.

\section{Query Processing}

The central goal of our work is to enable users to query structured knowledge bases without expecting \emph{any} schema knowledge whatsoever. In the context of the ER query model described previously, this means that keywords supplied in a user query will not in general match those used for data representation in the underlying data source. For example, a user might use the relation keyword \texttt{"established"} to mean ``establishment" of a company or an organization; but in the data source, the actual predicate sought could be \texttt{"founded"}---which roughly means the same as \texttt{"established"} in case of companies. 

Therefore, to achieve superior recall, we must explore many possible \emph{meanings} of a keyword used by the user. 

Further, a user specified keyword could be ambiguous, in the sense that it could could mean any of a number of entities/predicates in the data. To achieve superior precision, we must be able to disambiguate the intent of the user query. 

Our approach towards the above two problems is of asking the user for help. More concretely, our query processing engine shall prompt the user to select a subset of intermediate \emph{candidate} that entities or predicates that the system might be evaluating during query processing. 

Overall, the query processing proceeds with the repeated execution of two steps: 1) Predicate Selection, and 2) Entity Selection.

\subsection{Predicate selection}

As we assume that the user has no familiarity with representation of the underlying data, the relation keywords $R_{ij}$ specified by the user can not, in general, identify a unique predicate in the RDF graph. Therefore, in this stage, the system presents a set of candidate predicates that match the given relation keywords $R_{ij}$. The user is then expected to select of subset of these predicates that align with her intent. It is to be ensured that a small enough, yet accurate enough set of candidate predicates is presented to the user, in order to minimize user effort. This step, referred to as predicate selection for $p_{ij}$, is performed for each of the relations $x_i \sim x_j$ specified initially by the user.

\subsection{Entity selection}

After the user has selected desired set of predicates $\{p^1_{ij}, p^2_{ij}, ..., p^r_{ij}\}$ that comply with the given relation keywords $R_{ij}$, the system shall try to determine the entities $x_i$ and $x_j$ obeying any of these relations.

The RDF graph is queried to find all entities $x_i$ and $x_j$ that match the triple pattern $\langle ?x_i, p^k_{ij}, ?x_j \rangle$ ($k = 1,...,r)$. By the sets $X_i$ and $X_j$, we mean the set of candidate entities thus obtained for entity variables $x_i$ and $x_j$, respectively. These sets $X_i$ and $X_j$ are further filtered, based on the category keywords $C_i$, $C_j$, and selector keywords $K_i$, $K_j$. The resulting sets $\tilde{X_i}$ and $\tilde{X_j}$ obtained after filtering are presented as candidate entities for entity variables $x_i$ and $x_j$.

At this step, the user is expected to select a subset of these entities that align with her intent. As in the case of predicate selection, it is to be ensured that a small enough, yet accurate set of candidate entities is presented to the user. The entity selection step for entity variables $x_i$ and $x_j$ is performed after the predicate selection step for $p_{ij}$.

Once the user agrees on a set of candidate entities $\tilde{X_i}$ for entity variable $x_i$, this information will be used to reduce the complexity of the further entity selection stages. This process terminates when predicate selection and entity selection has been performed for each predicate and entity in the query. The entity-relation graphs are finally ranked and returned to the user as query results.


\section{Query Interpretation}
Even when we depend on the user's input for disambiguation of the query, we cannot simply blurt out a large set of candidate entities/predicates, for might the user be overwhelmed. Thus, we face the problem of giving an intermediate ranking to candidate entities/predicates, which can then be used to lower down the size of candidate set exposed to the user (in a top-k fashion).

\subsection{Predicate Selection}
A naive approach to populate the set of candidate predicates, would be to scan all possible predicates and discard those whose \emph{descriptions} do not match with the specified relation keywords $R_{ij}$. It is also possible to create a full text index on the predicate descriptions and to query it using $R_{ij}$. As can be expected, this approach is too simple and yields a large set of candidate predicates.

The choice of candidate predicates $p_{ij}$ can be improved by using the category keywords $C_i, C_j$ and the selector keywords $K_i, K_j$ for the entity variables $x_i$ and $x_j$ that are related by $p_{ij}$. Following the success of generative models in entity search \cite{sawant2013learning}, we propose a generative language model for joint query interpretation for choosing candidate predicates.

Let $\vec{q} = (R_{ij}, C_i, C_j, K_i, K_j)$ denote the part of the query pertaining to the predicate $p_{ij}$ and entities $x_i$, $x_j$. We are interested in $\argmax_{p}$Pr$(p|\vec{q})$, where
\begin{align}\label{eq:1}
\text{Pr}(p|\vec{q}) &\propto \text{Pr}(p, \vec{q}) = \text{Pr}(p)\cdot \text{Pr}(\vec{q}|p) = \text{Pr}(p)\cdot \sum_{t_{i}, t_{j}} \text{Pr}(\vec{q}, t_i, t_j | p) \\
&= \text{Pr}(p)\cdot \sum_{t_{i}, t_{j}} \text{Pr}(R_{ij}, C_i, C_j, K_i, K_j, t_i, t_j| p) \\
&= \text{Pr}(p)\cdot \sum_{t_{i}, t_{j}} \text{Pr}(R_{ij}|p)\cdot\underbrace{\text{Pr}(t_i,t_j|R_{ij},p)}_\text{(I)}\cdot\underbrace{\text{Pr}(C_i,C_j|t_i,t_j,R_{ij},p)}_\text{(II)}\cdot \phi_1 \label{eq:omphi1}
%&= \text{Pr}(p)\sum_{t_i, t_j, e_i, e_j} \text{Pr}(R_{ij}|p)\text{Pr}(t_i,t_j|R_{ij},p)\text{Pr}(e_i, e_j|t_i, t_j, R_{ij}, p) \text{Pr}(C_i,C_j|e_i,e_j,t_i,t_j,R_{ij},p)
\end{align}
where $\phi_1$ is defined as:
\begin{align}
\phi_1(K_i,K_j,C_i,C_j,e_i,e_j,t_i,t_j,R_{ij},p) = \text{Pr}(K_i,K_j|C_i,C_j,e_i,e_j,t_i,t_j,R_{ij},p) \label{eq:phi1}
\end{align}
Latent variables $t_i, t_j$ are introduced in (\ref{eq:1}), representing the types of the entities $x_i$ and $x_j$ respectively. The expressions (I) and (II) can be simplified, using the following reasonable independence assumptions:

\begin{enumerate}
\item $\text{Pr}(t_i,t_j|R_{ij},p) = \text{Pr}(t_i,t_j|p)$, i.e., the relation keywords $R_{ij}$ do not convey any extra information when the predicate $p$ is known.

\item $\text{Pr}(C_i,C_j|t_i,t_j,R_{ij},p) = \text{Pr}(C_i,C_j|t_i,t_j)$, i.e., the category keywords are independent of $R_{ij}$ and $p$, once $t_i$ and $t_j$ are known.

\item $\text{Pr}(C_i,C_j|t_i,t_j) = \text{Pr}(C_i|t_i) \cdot \text{Pr}(C_j|t_j)$, i.e., $C_i$ and $C_j$ are independent of each other and of any types other than $t_i$ and $t_j$, respectively.  
\end{enumerate}

Thus, we now have:
\begin{align}
\text{Pr}(p|\vec{q}) &\propto \text{Pr}(p)\cdot \sum_{t_{i}, t_{j}} \text{Pr}(R_{ij}|p)\cdot \text{Pr}(t_i,t_j|p) \cdot \text{Pr}(C_i|t_i) \cdot \text{Pr}(C_j|t_j) \cdot \underbrace{\phi_1}_\text{(III)} \label{eq:main2}
\end{align}

Furthermore, by introducing the latent variables $e_i$ and $e_j$ for entities represented by $x_i$ and $x_j$ in the query, we can rewrite $\phi_1$ as:
\begin{align}
\phi_1 = \text{Pr}(K_i,K_j|&C_i,C_j,t_i, t_j,R_{ij},p) = \sum_{e_i,e_j}\text{Pr}(K_i,K_j,e_i,e_j|C_i,C_j,t_i,t_j,R_{ij},p) \\
&= \sum_{e_i,e_j} \underbrace{\text{Pr}(e_i,e_j|C_i,C_j,t_i,t_j,R_{ij},p)}_\text{(IV)} \cdot \underbrace{\text{Pr}(K_i,K_j|e_i,e_j,C_i,C_j,t_i,t_j,R_{ij},p)}_\text{(V)} \label{eq:ugly2}
\end{align}

Similarly as before, we need the following independence assumptions to simplify (IV) and (V),
\begin{enumerate}
\item $\text{Pr}(e_i,e_j|C_i,C_j,t_i,t_j,R_{ij},p) = \text{Pr}(e_j,e_j|t_i,t_j,p)$, i.e, the category keywords $C_i,C_j$, and the relation keywords $R_{ij}$ do not convey any extra information when the types $t_i,t_j$, and the predicate $p$ is known.

\item $\text{Pr}(K_i,K_j|e_i,e_j,C_i,C_j,t_i,t_j,R_{ij},p) = \text{Pr}(K_i,K_j|e_i,e_j)$, i.e., the selector keywords $K_i,K_j$ are independent of category keywords $C_i,C_j$, the types $t_i,t_j$, the relation keywords $R_{ij}$, and the predicate $p$, once the actual entities $e_i,e_j$ are known.

\item $\text{Pr}(K_i,K_j|e_i,e_j) = \text{Pr}(K_i|e_i)\cdot \text{Pr}(K_j|e_j)$, i.e., $K_i$ and $K_j$ are independent of each other and of any entities other than $e_i$ and $e_j$, respectively.
\end{enumerate}

Therefore, (\ref{eq:ugly2}) reduces to $\sum_{e_i,e_j} \text{Pr}(e_i,e_j|t_i,t_j,p) \cdot \text{Pr}(K_i|e_i) \cdot \text{Pr}(K_j|e_j)$ and (\ref{eq:main2}) now becomes:

\begin{align}
\text{Pr}(p|\vec{q})& \propto \text{Pr}(p) \cdot \sum_{t_i,t_j,e_i,e_j} \phi_2(p, R_{ij}, C_i, C_j, K_i, K_j, t_i, t_j, e_i, e_j) \label{eq:main3}
\end{align}
where $\phi_2$ is defined as,
\begin{align}
\phi_2 = \text{Pr}(R_{ij}|p) \cdot \text{Pr}(t_i,t_j|p) \cdot \text{Pr}(C_i|t_i) \cdot \text{Pr}(C_j|t_j) \cdot \text{Pr}(e_i,e_j|t_i,t_j,p) \cdot \text{Pr}(K_i|e_i) \cdot \text{Pr}(K_j|e_j) \label{eq:phi2}
\end{align}
Arguments to functions $\phi_1$ and $\phi_2$ are omitted in Equations \ref{eq:omphi1}, \ref{eq:main2}, \ref{eq:ugly2} and \ref{eq:phi2}  for the lack of horizontal space.

Throughout this section we have dealt with probabilities and have ultimately arrived at a set of probabilities to be computed. We understand that such probabilities in the Bayesian sense of the word are hard to estimate, if at all well defined. We therefore, tweak our notation a little, replacing probabilities $\text{Pr}$ with potentials $\psi$. The entire derivation continues to hold, even with the modified semantics. 

For the rest of this chapter, we use the following notation:
\begin{enumerate}
  \item $\psi(p)$ in place of $\text{Pr}(p)$: represents the prior weight on the predicate $p$
  \item $\psi(R_{ij}, p)$ in place of $\text{Pr}(R_{ij}|p)$: represents the \emph{compatibility} of keywords $R_{ij}$ and predicate $p$
  \item $\psi(C_i, t_i)$ in place of $\text{Pr}(C_i|t_i)$: represents the \emph{compatibility} of keywords $C_i$ and a Freebase type $t_i$
  \item $\psi(K_i, e_i)$ in place of $\text{Pr}(K_i|e_i)$: represents the \emph{compatibility} of keywords $K_i$ and the Freebase entity $e_i$
  \item $\psi(t_i, p, t_j)$ in place of $\text{Pr}(t_i, t_j|p)$: represents the \emph{compatibility} of the predicate $p$ with the Freebase types $t_i$ and $t_j$
  \item $\psi(e_i, e_j, t_i, t_j, p)$ in place of $\text{Pr}(e_i, e_j | t_i, t_j, p)$: represents the \emph{compatibility} of Freebase entities $e_i$ and $e_j$ with Freebase types $t_i$ and $t_j$ respectively, and of the entities with the Freebase predicate $p$
\end{enumerate}

The potentials $\psi$ used above can be compactly represented as an undirected probabilistic graphical model. We omit the discussion towards this direction for brevity and proceed straight to describing how the above quantities will be estimated.

\subsubsection{Estimating $\psi(p)$}
$\psi(p)$ is simply the prior on the predicate $p$. We estimate it as:
$$\psi(p) = \frac{\text{freq}(p)}{\sum_{p_i \in \mathbb{P}}\text{freq}(p_i)}$$
where $\mathbb{P}$ is the set of all possible predicates in our RDF data source. 

\subsubsection{Estimating $\psi(R_{ij}, p)$, $\psi(C_i, t_i)$, and $\psi(K_i, e_i)$}
The  potentials $\psi(R_{ij}, p)$, $\psi(C_i, t_i)$, and $\psi(K_i, e_i)$ are similar in the sense that they all capture the agreement between two variables: one of them being keywords supplied in the user query and the other being a predicate, type, or an entity in the structured data source. 
For example, if the the relation keywords $R_{ij}$ are \texttt{"death reason"} and the related entity sought is of type \texttt{person}, then $R_{ij}$ should match to the predicate \texttt{/people/} \texttt{deceased\_person/} \texttt{cause\_of\_death} rather than the predicate \texttt{/people/} \texttt{deceased\_person/} \texttt{place\_of\_death}.

There may exist considerable variation in how keywords are represented in a query. The relation language model needs to build a bridge between the formal $p$ and the textual $R_{ij}$, so that (un)likely $p$'s have (small) large potential. Plenty of approaches \cite{berant2013semantic, berant2014semantic, kwiatkowski2013scaling, yih2014semantic} to this problem have been studied recently. We choose a pattern-based approach akin to PATTY \cite{nakashole2012patty} for its simplicity and scalability to large amounts of data.

For each Freebase predicate $p$, we discover the most strongly associated phrase patterns from a reference corpus, then mark these patterns into much larger payload corpus. 
The ClueWeb09 corpus annotated with Freebase entities \cite{gabrilovich2013facc1} may be used for this purpose. For each Freebase predicate $p$, we consider all triples and locate all the corpus sentences that mention both the participating entities of the triple. We assume, rather crudely, that if the entities $e_i$ and $e_j$ co-occur in a sentence then this sentence is an evidence of the triple $\langle e_i, p, e_j \rangle$. Each such sentence is parsed to obtain a dependency graph using the Malt Parser \cite{nivre2007maltparser}

We present the approach towards estimating $\psi(R_{ij}, p)$; those for $\psi(C_i, t_i)$ and $\psi(K_i, e_i)$ will be similar and shall be added in later versions of this work. The following brief description is based on \cite{joshiknowledge}.

Words in the path connecting the entities are joined together and added to a candidate phrase dictionary, provided the path is at most k-hops (k is chosen experimentally, we expect that longer dependency paths arise out of noisy sentences/parsing). Thus, we have, for all predicates $p$, a list $\mathbb{R}_{ij}$ of all phrases that are known to hint at $p$. We can then estimate $\psi(R_{ij}, p)$ as:
\begin{align}
\psi(R_{ij}, p) = \frac{n(p, R_{ij})}{\sum_{R'_{ij} \in \mathbb{R}_{ij}}{n(p, R'_{ij})}}
\end{align}
 where $n(p, R_{ij})$ represents the number of sentences in which $R_{ij}$ occurred as a phrase in the dependency path between entities participating in relation specified by predicate $p$.

\subsubsection{Estimating $\psi(t_i,t_j,p)$}
As the potential does not involve any query keywords, it is relatively straightforward to estimate. For the predicate $p$, we identify all the triples that it is involved in, and filter those out  in which the connected subject and object have types $t_i$ and $t_j$ respectively. Let $n(p; t_i, t_j)$ be the number of such triples. Then, simply

\begin{align}
\psi(t_i,t_j,p) = \frac{n(p; t_i, t_j)}{n(p)}
\end{align}
where $n(p)$ denotes the number of triples in Freebase that have $p$ as the predicate.

Note that the filtering operation above can be easily carried out in case of Freebase due to the availability of a \texttt{/object/type/} relation for most entities.

\subsubsection{Estimating $\psi(e_i,e_j,t_i,t_j,p)$}
Though $\psi(e_i,e_j,t_i,t_j,p)$ must take into account the $e_i$--$t_i$, and $e_j$--$t_j$ compatibility, we make a simplifying assumption without deviating a lot from the intended semantics of $\psi(e_i,e_j,t_i,t_j,p)$.

Define, 
\begin{align}
\psi(e_i,e_j,t_i,t_j,p) = \frac{n(e_i, e_j; p)}{n(p)} \label{eq:psi4}
\end{align}
where $n(e_i, e_j; p) = 1$ if the triple $\langle e_i, p, e_j \rangle$ occurs in the data, otherwise $n(e_i, e_j; p) = 0$. As earlier, $n(p)$ denotes the number of triples that have $p$ as their predicate part.

\subsubsection{Computation}
Now that we have defined the quantities involved in Equation \ref{eq:main3}, we can compute the summation involved on the right hand side. Note that only $\psi(e_i, e_j | t_i, t_j, p)$ involves all the summation variables $t_i, t_i, e_i, e_i$ and can be expensive to compute. The summation on the other terms can be computed rather cheaply.

\begin{comment}
Looking back at our definition of $\psi(e_i, e_j | t_i, t_j, p)$ in Equation \ref{eq:psi4}, 

\begin{align}
\sum_{t_i, t_j, e_i, e_j} \psi(e_i, e_j | t_i, t_j, p) & = \sum_{t_i, t_j, e_i, e_j} \frac{n(e_i, e_j; p)}{n(p)} \nonumber \\
 & = \sum_{e_i, e_j} \frac{n(e_i, e_j; p)}{n(p)} \nonumber \\
 & = 1
\end{align}

which means that the factor $\psi(e_i, e_j | t_i, t_j, p)$ can be directly dropped.
\end{comment}

One way of reducing the computation costs, is to plainly set $\psi(e_i, e_j | t_i, t_j, p) = 1$. The burden of producing a valid \emph{score} rests divided on all the $\psi$'s involved, and we expect that the results will not be perturbed much on ignoring one of the $\psi$'s.

Also, the argmax computation that we started off initially with can be replaced by a \emph{score} computation for each of the predicates. On cleaning our Freebase dataset we ended up with only 6200 distinct predicates, so the score computation will be generally feasible, given that several of the quantities can be precomputed and stored.

\subsection{Entity Selection}
Similar to our above approach towards Predicate Selection, we use a generative language model to narrow down candidate entities for the Entity Selection stage.

Let $e_i$ denote a candidate entity (unknown) for the entity variable $x_i$ and let $\vec{q_{e_i}} = (K_i, C_i)$ be the part of the query pertaining directly to $x_i$. As before, we are interested in $\argmax_{e} \text{Pr}(e|\vec{q_{e_i}})$, where
\begin{align}
\text{Pr}(e|\vec{q_{e_i}}) \propto \text{Pr}(e, \vec{q_{e_i}}) & = \text{Pr}(e) \cdot \text{Pr}(\vec{q_{e_i}}|e)  \\
\label{eq:ent1}  &= \text{Pr}(e) \cdot \sum_{p_{ij_1}, p_{ij_2},..., p_{ij_r}}\text{Pr}(\vec{q_{e_i}}, p_{ij_1}, p_{ij_2},..., p_{ij_r} | e) \\
 &= \text{Pr}(e) \cdot \sum_{p_{ij_1}, p_{ij_2},..., p_{ij_r}}\text{Pr}(K_i, C_i, p_{ij_1}, p_{ij_2},..., p_{ij_r} | e)  \nonumber \\
  &= \text{Pr}(e) \cdot \sum_{p_{ij_1}, p_{ij_2},..., p_{ij_r}} \text{Pr}(K_i | e) \cdot \underbrace{\text{Pr}(C_i | K_i, e)}_\text{(VI)} \cdot \underbrace{\text{Pr}(p_{ij_1}, p_{ij_2},..., p_{ij_r} | C_i, K_i, e)}_\text{(VII)} \label{eq:entmain2}
\end{align}

The summation of (\ref{eq:ent1}) is obtained by the introduction of latent variables $p_{ij_1}, p_{ij_2},..., p_{ij_r}$, that denote the various predicates that relate $x_i$ to any other $x_j (i \neq j)$ in the query. Note that we arrive at the entity selection stage only after one or more stages of predicate selection. Thus, the variables $p_{ij_1}, p_{ij_2},..., p_{ij_r}$ range over $\mathbb{P}_{ij_1}, \mathbb{P}_{ij_2}, ...,\mathbb{P}_{ij_r}$ respectively, where $\mathbb{P}_{ij_k} (1 \le k \le r)$ denote the set of selected candidate predicates in previous predicate selection stage(s). $\mathbb{P}_{ij_k} = \emptyset$, in case predicate selection stage has not occurred for $p_{ij_k}$, yet.

As previously, we make several reasonable independence assumptions to simplify (VI) and (VII),
\begin{enumerate}
\item $\text{Pr}(C_i|K_i,e) = \text{Pr}(C_i|e)$, i.e., given the entity $e$, the category keywords $C_i$ are independent of the selector keywords $K_i$.

\item $\text{Pr}(p_{ij_1}, p_{ij_2},..., p_{ij_r} | C_i, K_i, e) = \prod_{k=1}^{r} \text{Pr}(p_{ij_k} | e)$, i.e., the predicates are independent of each other and $C_i, K_i$, once the entity $e$ is known.
\end{enumerate}

By now, Equation \ref{eq:entmain2} has been reduced to:
\begin{align}
\text{Pr}(e, \vec{q_{e_i}}) &= \text{Pr}(e) \cdot \text{Pr}(K_i | e) \cdot \text{Pr}(C_i | e) \cdot \sum_{p_{ij_1}, p_{ij_2},..., p_{ij_r}}  \prod_{k=1}^{r} \text{Pr}(p_{ij_k} | e) \label{eq:entmain3}
\end{align}

As previously done for the case of entity selection, we change the semantics of the above equation and introduce $\psi$ potential functions instead of probability mass function $Pr$. We now proceed towards computing the various $\psi's$ involved.

\subsubsection{Estimating $\psi(e)$}
$\psi(e)$ is simply the prior on entity $e$. We estimate it as:
$$\psi(e) \approx \frac{\text{freq}(e)}{\sum_{e_i \in \mathbb{E}}\text{freq}(e_i)}$$
where $\mathbb{E}$ is the set of entities considered for ranking. 

\subsubsection{Estimating $\psi(K_i, e)$}
An initial approach is to use WordNet synsets of the short description available for the entity $e$; and set $\psi(K_i, e)$ for any matching words in the synsets. With the availability of entity-annotated corpora and on-the-fly entity annotators \cite{ferragina2010tagme}, more sophisticated measures can be defined.

\subsubsection{Estimating $\psi(C_i, e)$}
A Freebase entity can in general, be associated with several types $t_1, ..., t_m$. We define, 
\begin{align}
 \psi(C_i, e) = \sum_{j=1}^{m} \psi(C_i, t_j)
\end{align}
where $\psi(C_i, t_j)$ can be used as discussed for predicate in the previous section.

\subsubsection{Estimating $\psi(p_{ij_{k}}, e)$}
It is natural to have the following definition,
\begin{align}
\psi(p_{ij_{k}}, e) = \frac{n(p_{ij_{k}}, e)}{n(e)}
\end{align}
where $n(p_{ij_{k}}, e)$ is the number of triples that have $e$ related by the predicate $p_{ij_{k}}$, and $n(e)$ is the total number of triples of $e$.

\subsubsection{Computation}
The summation in Equation \ref{eq:entmain3}, when rewritten in product-of-sum form, is not expected to be computationally expensive owing to the following observations: 
\begin{enumerate}
  \item The predicates $p_{ij_1}, p_{ij_r}, ..., p_{ij_r}$ are the predicates that concern the entity variable $e_i$ in the query. Therefore, for even the most convoluted (and meaningful) queries one can think of, $r$ would be a small number ($< 10$ let's say).
  \item The predicate selection step would have been completed for these predicates before entity selection is done for the entity variable $e$. This limits the total number of terms in the sum, allowing for cheap computation of the overall \emph{score}.   
\end{enumerate}

Further success can be had by precomputing some of the quantities $\psi$. 

The argmax computation can be done away with, and top-k ranking entities can be chosen as the candidates to be presented to the user. Even the score is not required to be computed for all entities and this step can be assisted by using a heuristic to filter a larger set of candidate entities; and then computing the ranking for these entities. An initial heuristic would be to perform a keyword search using the selector keywords (and their synonyms) of the entity and to perform ranking for the result entities. Keyword search can be supported by systems like Apache Solr \cite{smiley2015apache}.

\section{Result of the query}

At this stage, we have a set of selected entities for each entity variable sought by the query, and also a set of selected predicates for each of the relations between pairs entity variables in the query. We can further assume with some confidence that sizes of these set would not be too large. In such a situation, arriving at the result set is a matter of computing the join of selected entity sets corresponding  to adjacent entities in the query-graph. Further filtering may be required to discard (entity-predicate-entity) tuples that do not have matching triples in the data.

